\documentclass[12pt,a4paper]{amsart}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{float}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage[margin=1in]{geometry}
\usepackage{subfig}
\captionsetup{belowskip=12pt,aboveskip=2pt}


\title{Computer Vision 1 - Final Project}

\author{
Sharon Gieske, Elise Koster, David van Erkelens
}
\title{Automated Image Classification}
\usepackage{fancyhdr}
\usepackage{booktabs}

% Redefine section macros
\makeatletter
\def\section{%
  \@startsection{section}{1}{\z@}%
  {1.5\linespacing\@plus\linespacing}%
  {1.3\linespacing}%
  {\bfseries\normalfont\scshape}
}
\def\subsection{\@startsection{subsection}{2}{\z@}%
  {.5\linespacing\@plus.7\linespacing}%
  {.5\linespacing}%
  {\small\normalfont}}
\makeatother

% Set par indents to 0
\setlength{\parindent}{0cm}

% Set header for every page (except for first page)
\pagestyle{fancy}
\rhead[\small\textsc{Elise Koster, Sharon Gieske, David van Erkelens}]{\small\textsc{Automated Image Classification}}
\lhead{\thepage.}
\cfoot{}
\date{}
\begin{document}
\begin{titlepage}
\begin{center}
    \includegraphics[width=\textwidth]{logo.png}
    \\ [2.5cm]
    \textsc{\Large Computer Vision 1}
    \\ [0.5cm]
    \textsc{\large Final Assignment}
    \\ [1cm]
    \hrule
    \vspace{0.3cm}
    \textsc{Automated Image Classification}
    \\ [0.3cm]
    \hrule
    \vfill
    \textsc{Elise Koster, 5982448 \\ Sharon Gieske, 6167667 \\ David van Erkelens, 10264019 \\[0.7cm] Master's Programme Artificial Intelligence \\Graduate School of Informatics \\ University of Amsterdam \\[0.4cm] \today}
\end{center}
\end{titlepage}

\section{Introduction}
Object classification is a fundamental part of Computer Vision and can be used to automate processes and provide environmental reasoning for robotics. Consequently, a high accuracy in object classification systems is invaluable for industry and science alike.\\
This paper outlines the results of a project analyzing different approaches to image classification using techniques from Machine Learning and Computer Vision. Multiple techniques have been implemented and tested, yielding different results.\\
The data section will describe the images used for training and testing, the implementation section will introduce the techniques used and the results section will describe the difference in performance for each set of techniques. The conclusion will report on the optimal combination found and the section future work will comment on possible improvements.

\section{Data}
The training data consists of 2000 .jpg-images in four classes (500 per class): airplanes, cars, faces and motorbikes. The group of training images is split into a vocabulary-training set and a classification training set.The test data consists of 200 .jpg-images in the same four classes, all of whom are used for testing.

\section{Implementation}
Instead of training classifiers on a large set of pixels, the bag-of-words approach is used. This approach first extracts features from images and subsequently uses them to build a vocabulary of visual `words'. Each image can then be described as a set of these words, which makes training a classifier easier and faster than a pixel-by-pixel approach. \\
\subsection{SIFT}
To be able to build a bag-of-words, features need to be extracted from each training image. This is done using Scale Invariant Feature Transform (SIFT), an algorithm that detects points of interest in an image and produces descriptors of these features. Two types of SIFT are used: key-point (produces descriptors of points of interest) and dense-sampling (every $n$ pixels a descriptor is produced). A multitude of color spaces is used: gray-scale, RGB (regular .jpg-image with three channels), normalized RGB (normalizedRGB) and opponent, where $R,G,B$ respectively are the pixel values per color channel, and
\begin{align*}
\text{For normalizedRGB:}\\
r&= \frac{R}{R+G+B}\\
g&=\frac{G}{R+G+B} \\
b&= \frac{B}{R+G+B}\\
\text{For opponent:}\\
O_1&= \frac{R-G}{\sqrt{2}}\\
O_2&= \frac{R+G-2B}{\sqrt{6}}\\
O_3&= \frac{R+G+B}{\sqrt{3}}
\end{align*}
Each different color space defines different intensities for each pixel in a color channel, and thus causes SIFT to return different descriptors.
\subsection{K-means}
Performing SIFT on the first set of training images results in a set of descriptors for each image, which are clustered into visual words using K-means. The K-means algorithm works by calculating the Euclidian distance between each data point and a cluster-center (the mean), and iteratively re-calculating the means and re-assigning the data-points until convergence. The resulting clusters form the visual vocabulary used for describing and classifying images later on. 
\subsection{SVM classification}
Once the visual vocabulary has been built, features are extracted from a new set of training images. These features are grouped into words according to the visual vocabulary, and for each image a histogram of visual word frequencies is computed.
These histograms are used as input to train four Support Vector Machines (SVMs) (one per class), using different kernel-functions. SVMs are non-parametric classifiers, which work by maximizing the margin between the decision boundary and two classes of data.
After training, all test images are classified according to the SVM-models built using the training images. 
\subsection{Evaluation}
The classification systems are evaluated using the Average Precision (AP) for each class and Mean Average Precision (MAP) over all classes. The Average Precision for a single class is defined as
\begin{align}
\frac{1}{m_c} \sum\limits_{i=1}^N\frac{f_c(x_i)}{i}
\end{align}
where $N$ is the total number of images, $m_c$ is the number of images of class $c$, $x_i$ is the ith image in the ranked list and $f_c$ is a function which returns the number of images of class $c$ in the first $i$ images if $x_i$ is of class $c$, and 0 otherwise.

\section{Supplied files}
\input{Supplied_files}

\section{Results}
This section contains an overview and short analysis of the different results obtained during this project. \\
\input{results}

\section{Conclusion}

\section{Future work}
It is hypothesized that for dense SIFT, different step sizes will work better with different window sizes. For example, in small window sizes, the step sizes need to be smaller to allow for the capturing of all local features, whereas for larger window sizes, these steps can be smaller while still capturing all required features.

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
