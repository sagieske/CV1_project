\documentclass{article} % For LaTeX2e
\usepackage{times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{times}
\usepackage{tikz}
\usepackage[]{algorithm2e}
\usetikzlibrary{arrows,decorations.pathmorphing,fit,positioning}
\usepackage{algorithmic}
\usepackage{mdframed}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Image classification using bag-of-words model}


\author{
Sharon Gieske \\
\texttt{6167667}\\
\texttt{sharongieske@gmail.com} \\
\and
David van Erkelens\\
\texttt{10264019}\\
\texttt{daviddvanerkelens@gmail.com} \\
\and
Elise Koster \\
\texttt{5982448}\\
\texttt{koster.elise@gmail.com}
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\begin{document}


\maketitle

\begin{abstract}
A bag-of-words approach to image classification was used in four different image categories. After exploring the parameter space, the best model reports a mean average precision of over 97\%.
\end{abstract}


\section{Introduction}
Object classification is a fundamental part of Computer Vision and can be used to automate processes and provide environmental reasoning for robotics. Consequently, a high accuracy in object classification systems is invaluable for industry and science alike.\\
This paper outlines the results of a project analyzing a bag-of-words approach to image classification, and explores the effectiveness of different parameter settings.\\
The data section will describe the images used for training and testing, the implementation section will introduce the techniques used and the results section will describe the difference in performance for each set of techniques. The conclusion will report on the optimal combination found and the section future work will comment on possible improvements.

\section{Data}
The training data consists of 2000 .jpg-images in four classes (500 per class): airplanes, cars, faces and motorbikes. The group of training images is split into a vocabulary training set and a classification training set.The test data consists of 200 .jpg-images in the same four classes, all of whom are used for testing.

\section{Implementation}
Instead of training classifiers on a large set of pixels, the bag-of-words approach is used. This approach first extracts features from images and subsequently uses them to build a vocabulary of visual `words'. Each image can then be described as a set of these words, which makes training a classifier easier and faster than a pixel-by-pixel approach. \\
\subsection{SIFT}
To be able to build a bag-of-words, features need to be extracted from each training image. This is done using Scale Invariant Feature Transform (SIFT), an algorithm that detects points of interest in an image and produces descriptors of these features. Two types of SIFT are used: key-point (produces descriptors of points of interest) and dense-sampling (for every $n$ pixels a descriptor is produced). A multitude of color spaces is used: gray-scale, RGB (regular .jpg-image with three channels), normalized RGB (normalizedRGB) and opponent, where $R,G,B$ respectively are the pixel values per color channel, and
\begin{align*}
\text{For normalizedRGB:}\\
r&= \frac{R}{R+G+B}\\
g&=\frac{G}{R+G+B} \\
b&= \frac{B}{R+G+B}\\
\text{For opponent:}\\
O_1&= \frac{R-G}{\sqrt{2}}\\
O_2&= \frac{R+G-2B}{\sqrt{6}}\\
O_3&= \frac{R+G+B}{\sqrt{3}}
\end{align*}
Each different color space defines different intensities for each pixel in a color channel, and thus causes SIFT to return different descriptors.
\subsection{K-means}
Performing SIFT on the first set of training images results in a set of descriptors for each image, which are clustered into visual words using K-means. The K-means algorithm works by calculating the Euclidian distance between each data point and a cluster-center (the mean), and iteratively re-calculating the means and re-assigning the data-points until convergence. The resulting clusters form the visual vocabulary used for describing and classifying images later on. 
\subsection{SVM classification}
Once the visual vocabulary has been built, features are extracted from a new set of training images. These features are grouped into words according to the visual vocabulary, and for each image a histogram of visual word frequencies is computed.
These histograms are used as input to train four Support Vector Machines (SVMs) (one per class), using different kernel-functions. SVMs are non-parametric classifiers, which work by maximizing the margin between the decision boundary and two classes of data.
After training, all test images are classified according to the SVM-models built using the training images. 
\subsection{Evaluation}
The classification systems are evaluated using the Average Precision (AP) for each class and Mean Average Precision (MAP) over all classes. The Average Precision for a single class is defined as
\begin{align}
\frac{1}{m_c} \sum\limits_{i=1}^N\frac{f_c(x_i)}{i}
\end{align}
where $N$ is the total number of images, $m_c$ is the number of images of class $c$, $x_i$ is the ith image in the ranked list and $f_c$ is a function which returns the number of images of class $c$ in the first $i$ images if $x_i$ is of class $c$, and 0 otherwise.

\section{Supplied files}
\input{Supplied_files}

\section{Results}
This section contains an overview and short analysis of the different results obtained during this project. \\
\input{results}

\section{Conclusion}
From the results shown in this report, it is notable that image classification using SIFT shown promising results. Even though not every permutation of the possible parameters have been tested, an average mean precision of over 97\% has already been achieved. \\
From the results it can be noted that dense SIFT outperforms keypoint SIFT. This implies that features in an image are often relevant, even if they do not show up as a key point. The opponent color space was the most suitable color space to extract features from the images, slightly outperforming other color spaces like RGB and really outperforming color spaces like normalized RGB, which implies that normalization of colors also removes some of the features of importance to classify images on their context. \\ 
The amount of trainig images is also of great importance for the performance of the algorithm. When using a grayscale color space, 50 training images are sufficient to retrieve a mean average precision of about 80\%. However, then using a colored color space, the minimum number of training images to retrieve a mean average precision of about 80\% lies around 75. The effect of chaning the window size, step size and amount of clusters can also be seen in this report, for all of these parameters the optimum lies somewhere in the middle of the tested values. \\
Generally, it can be concluded that feature extracting using SIFT combined with binary SVM classifiers yields good performance for classifing images into four categories. The algorithm can be easily extended to include more classes by adding more binary classifiers. However, when more classes are included, the parameters have to be tweaked again to achieve optimal performance, and it can be questioned if an algorithm with more than four classes will achieve a performance as high as 97\%.

\section{Future work}
It is hypothesized that for dense SIFT, different step sizes will work better with different window sizes. For example, in small window sizes, the step sizes need to be smaller to allow for the capturing of all local features, whereas for larger window sizes, these steps can be smaller while still capturing all required features. \textbf{TODO: MOET EEN VAN DEZE TWEE SMALLERS NIET LARGER ZIJN?????}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
