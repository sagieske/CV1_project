\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage[compact,explicit]{titlesec}

\title{Computer Vision 1 - Final Project}

\author{
Sharon Gieske - 6167667\\
Elise Koster - 5982448\\
David van Erkelens -10264019\\
}

\date{}
\begin{document}
\maketitle
\section{Introduction}
%WHY ARE WE DOING THIS?
This paper outlines the results of a project analyzing different approaches to image classification using techniques from Machine Learning and Computer Vision. Multiple techniques have been implemented and tested, yielding different results.\\
The data section will describe the images used for training and testing, the implementation section will introduce the techniques used, the results section will describe the difference in performance for each set of techniques, and the conclusion will report on the optimal combination found.

\section{Data}
The training data consists of 2000 .jpg-images in four classes (500 per class): airplanes, cars, faces and motorbikes. The test data consists of 200 .jpg-images in the same four classes.

\section{Implementation}
Instead of training classifiers on a large set of pixels, the bag-of-words approach was used. This approach first extracts features from images and subsequently uses them to build a vocabulary of visual 'words'. Each image can then be described as a set of these words, which makes training a classifier easier and faster than a pixel-by-pixel approach. \\
First, features are extracted from a set of training images using key-point and dense-sampling SIFT, for gray-scale, RGB, normalized RGB (rgb) and opponent color spaces. This results in a set of descriptors for each train image, which are clustered into visual words using K-means. The resulting clusters form a visual vocabulary. \\
Then, features are extracted from a new set of training images. These features are grouped into words according to the visual vocabulary, and for each image a histogram of visual word frequencies is computed.\\
These histograms are used as input to train four SVM-classifiers (one per class), using different kernel-functions. After training, all test images are classified according to the SVM-models built using the training images.
%BAG OF WORDS 
%HOW DOES IT WORK:
%GET FEATURES/DESCRIPTORS USING SIFT
%DESCRIBE SIFT/RGBSIFT/rgbSIFT/opponentSIFT
%DESCRIBE FEATURES/DESCRIPTORS
%BUILD VISUAL VOCABULARY USING KMEANS
%DESCRIBE KMEANS
%DESCIBE VISUAL WORDS
%QUANTIZE FEATURES
%EACH IMAGE IS COLLECTION OF VISUAL WORDS
%EXTRACT FEATURES USING SIFT
%ASSIGN DESCRIPTOR TO CLOSEST VOCAB WORD
%CREATE HISTOGRAM OF VISUAL WORDS PER IMAGE
%TRAIN SVM ACCORDING TO HISTOGRAMS
%FOUR SVMS: 1 PER CLASS
%EVALUATION:
%USE TRAINED SVMS TO CLASSIFY TEST IMAGES
%COMPUTE MEAN AVERAGE PRECISION OVER ALL CLASSES
\section{Results}
Results of:\\
\begin{itemize}
\item key points vs dense
\item vocabulary size(400,800,1600,2000,4000)
\item SIFT color space (gray-scale, RGB, rgb, opponent)
\item amount of training samples used (vocab)
\item amount of training samples used (svm)
\item kernel choice for sum
\end{itemize}
\begin{tabular}{|c|c|c|}
\hline
SIFT type & color space & accuracy \\
\hline
x & y & z\\
a & b & c\\
\hline
\end{tabular}
\section{Conclusion}
\end{document}